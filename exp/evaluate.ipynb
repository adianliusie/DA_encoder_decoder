{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/alta/Conversational/OET/al826/2022/DA_classification/1.2-encoder_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 33.09it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9ce9ff92af484cbde20f579dbb66fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e2cd55d42e49aa99157375fb0935d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab5fffed5f549979d651ecccdc84dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.handlers.eval_handler import EvalHandler, EnsembleEvaluator\n",
    "from src.helpers.conv_handler import ConvHandler\n",
    "\n",
    "eval_path = 'swda/standard/test.json'\n",
    "#E = EvalHandler('baseline/2e-5/1')\n",
    "#probs, labels = E.predictions(eval_path=eval_path)\n",
    "\n",
    "Ens = EnsembleEvaluator('baseline/1e-5')\n",
    "probs, labels = Ens.predictions(eval_path=eval_path)\n",
    "\n",
    "#Ens = EnsembleEvaluator('baseline/1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import config \n",
    "label_names = ConvHandler.load_label_info(f'{config.base_dir}/data/{eval_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "    accuracy                          0.811      4514\n",
      "   macro avg      0.640     0.624     0.611      4514\n",
      "weighted avg      0.812     0.811     0.809      4514\n",
      "              precision    recall  f1-score   support\n",
      "    accuracy                          0.805      4514\n",
      "   macro avg      0.601     0.564     0.570      4514\n",
      "weighted avg      0.802     0.805     0.801      4514\n",
      "              precision    recall  f1-score   support\n",
      "    accuracy                          0.809      4514\n",
      "   macro avg      0.617     0.599     0.598      4514\n",
      "weighted avg      0.804     0.809     0.804      4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miproj/4thyr.oct2019/al826/anaconda3/envs/torch1.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/miproj/4thyr.oct2019/al826/anaconda3/envs/torch1.7/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class MutliClassEval:\n",
    "    @classmethod\n",
    "    def ensemble(cls, ensemble, labels, detail=False, names=None):\n",
    "        seed_probs = [np.concatenate(seed_probs, axis=0) for seed_probs in ensemble]\n",
    "        mean_probs = np.mean(seed_probs, axis=0)\n",
    "        flat_labels = np.concatenate(labels, axis=0)\n",
    "        cls.acc_report(mean_probs, flat_labels)\n",
    "\n",
    "    @classmethod\n",
    "    def seed_eval(cls, ensemble, labels, detail=False, names=None):\n",
    "        for seed in ensemble:\n",
    "            cls.seq_preds(seed, labels, detail, names)\n",
    "\n",
    "    @classmethod\n",
    "    def seq_preds(cls, probs, labels, detail=False, names=None):\n",
    "        flat_probs  = np.concatenate(probs, axis=0)\n",
    "        flat_labels = np.concatenate(labels, axis=0)\n",
    "        cls.acc_report(flat_probs, flat_labels)\n",
    "        \n",
    "    @staticmethod\n",
    "    def acc_report(probs, labels, detail=False, names=None):\n",
    "        decision = np.argmax(probs, axis=-1)\n",
    "        \n",
    "        if (names and detail):\n",
    "            decision = [names[i] for i in decision]\n",
    "            labels = [names[i] for i in labels]\n",
    "\n",
    "        report = metrics.classification_report(labels, decision, digits=3)  \n",
    "        \n",
    "        rows = report.split('\\n')\n",
    "        header = rows[0]\n",
    "        summary = '\\n'.join(rows[-4:-1])\n",
    "        \n",
    "        print(header)\n",
    "        if detail:\n",
    "            #This is all done to be able to sort by F1 (can change index to sort by P,R or support)\n",
    "            report = metrics.classification_report(labels, decision, digits=3, output_dict=True)  \n",
    "            results = [[k, v['precision'], v['recall'], v['f1-score'], v['support']] \\\n",
    "                       for k, v in report.items() if k not in ['weighted avg', 'macro avg', 'accuracy']] \n",
    "            results.sort(key=lambda x: x[3], reverse=True)\n",
    "            \n",
    "            width=max([len(x[0]) for x in results])\n",
    "            for row in results:\n",
    "                print(f' {row[0]:>{width}} {row[1]:>9.3f} {row[2]:>9.3f} {row[3]:>9.3f} {row[4]:>9}')\n",
    "            print()\n",
    "        print(summary)\n",
    "        \n",
    "MutliClassEval.seed_eval(probs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'E' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf2da4cb2aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'baseline/2e-5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'swda/standard/test.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'E' is not defined"
     ]
    }
   ],
   "source": [
    "Ens = EnsembleEvaluator('baseline/2e-5')\n",
    "probs, labels = E.predictions(eval_path='swda/standard/test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "eval_path = f\"swda/standard/test.json\"\n",
    "\n",
    "test_args = {'eval_path':eval_path,\n",
    "             'bsz':2,\n",
    "             'lim':None}\n",
    "\n",
    "test_args = SimpleNamespace(**test_args)\n",
    "E.evaluate(test_args)\n",
    "\n",
    "#words, scores = E.saliency(test_args, N=100, utt_num=63)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
