{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRF_decoder(nn.Module):\n",
    "    def __init__(self, num_class, h_size=768, t_weight=1):\n",
    "\n",
    "        self.transitions = \\\n",
    "            nn.Parameter(torch.randn(num_class, num_class))\n",
    "            \n",
    "        self.start_transitions = \\\n",
    "            nn.Parameter(torch.randn(1, num_class))\n",
    "        \n",
    "        self.classifier = nn.Linear(h_size, num_class)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_mask, labels):\n",
    "        emissision = self.classifier(encoder_outputs) #[B, N, C]\n",
    "        transition = self.classifier(encoder_outputs) #[B, N, C]\n",
    "        \n",
    "        \n",
    "    def forward_alg():\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(torch.nn.Module):\n",
    "    def __init__(self, embeddings, embedding_size, hidden_size, label_number):\n",
    "        super().__init__()\n",
    "        self.label_number = label_number\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(embeddings)\n",
    "        self.LSTM = torch.nn.LSTM(embedding_size, hidden_size, 1, False, True, 0, True)\n",
    "        self.classifier = torch.nn.Linear(2*hidden_size, 50, bias = False)\n",
    "        self.classifier_2 = torch.nn.Linear(50, label_number, bias = False)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "        self.transitions = torch.nn.Parameter(torch.randn(label_number*label_number,1))\n",
    "        self.lambd = torch.nn.Parameter(torch.randn(1))\n",
    "\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        #self.transitions = torch.FloatTensor([[10],[9],[5],[7]]).to(torch.device(\"cuda\"))\n",
    "        #self.transitions = torch.zeros(label_number*label_number,1).to(self.device)\n",
    "\n",
    "    def forward(self, sentence_ids, mask, labels):\n",
    "        context_tensor = self.encode(sentence_ids, mask)\n",
    "\n",
    "        emission_score_mat = self._get_emission_scores(context_tensor, mask)\n",
    "        sentence_score_mat = self._get_sentence_scores(emission_score_mat, labels)\n",
    "        normalising_mat = self._get_partition_function(emission_score_mat)\n",
    "        sentence_scores = torch.sum(torch.mul(sentence_score_mat, mask), dim=-1)\n",
    "        sum_sentence_score = torch.sum(sentence_scores, dim=-1)\n",
    "\n",
    "        sum_Z_score = torch.zeros(1).to(self.device)\n",
    "        mask_seq = torch.sum(mask.to(torch.long), dim=-1)\n",
    "        for i in range(len(normalising_mat)):\n",
    "            sequence = normalising_mat[i]\n",
    "            sum_Z_score += sequence[mask_seq[i]-1]\n",
    "\n",
    "        return -(sum_sentence_score - sum_Z_score)\n",
    "\n",
    "    def decode(self, sentence_ids, mask):\n",
    "        context_tensor = self.encode(sentence_ids, mask)\n",
    "        emission_score_mat = self._get_emission_scores(context_tensor, mask)\n",
    "\n",
    "        decision_matrix = torch.zeros(sentence_ids.shape[0], sentence_ids.shape[1]).to(self.device)\n",
    "        mask_sequence = torch.sum(mask, dim=1).int().tolist()\n",
    "        num_label = self.label_number\n",
    "        for i in range(len(sentence_ids)):\n",
    "            best_list = [[0, []] for _ in range(num_label)]\n",
    "            extended_list = [[0, []] for _ in range(num_label*num_label)]\n",
    "\n",
    "            for j in range(mask_sequence[i]):\n",
    "                emission = emission_score_mat[i][j]\n",
    "                for k in range(len(extended_list)):\n",
    "                    extended_list[k][0] = best_list[k//num_label][0] + self.transitions[k] + emission[k%num_label]\n",
    "                    extended_list[k][1] = best_list[k//num_label][1] + [k%num_label]\n",
    "\n",
    "                for m in range(len(best_list)):\n",
    "                    paths = [b for a,b in enumerate(extended_list) if a%num_label == m]\n",
    "                    best_list[m] = max([path for path in paths]).copy()\n",
    "\n",
    "            decoded_sequence = max(best_list)[1]\n",
    "            decision_matrix[i,:len(decoded_sequence)] = torch.FloatTensor(decoded_sequence)\n",
    "\n",
    "        sentence_score_mat = self._get_sentence_scores(emission_score_mat, decision_matrix)\n",
    "        cum_f_score = torch.cumsum(sentence_score_mat, dim=-1)\n",
    "        normalising_mat = self._get_partition_function(emission_score_mat)\n",
    "\n",
    "        shifted_f = torch.cat((torch.zeros_like(cum_f_score[:,-1:]), cum_f_score[:,:-1]),1)\n",
    "        shifted_Z = torch.cat((torch.zeros_like(normalising_mat[:,-1:]), normalising_mat[:,:-1]),1)\n",
    "\n",
    "        log_p = (cum_f_score - normalising_mat) - (shifted_f - shifted_Z)\n",
    "        probs = torch.exp(log_p.unsqueeze(-1))\n",
    "        result = probs * one_hot_encoding(decision_matrix)\n",
    "        result[:,:,0][result[:,:,0] == 0] = 1\n",
    "        result[:,:,0]  = 1 - result[:,:,0]\n",
    "        output = torch.sum(result, dim=-1)\n",
    "        return one_hot_encoding(output)\n",
    "\n",
    "    def encode(self, sentence_ids, mask):\n",
    "        input_tensor = self.embedding(sentence_ids)\n",
    "        mask_sequence = torch.sum(mask, dim=1)\n",
    "        jagged_input_tensor = torch.nn.utils.rnn.pack_padded_sequence(input_tensor, mask_sequence, batch_first = True, enforce_sorted = False)\n",
    "        jagged_context_embedding_tensor, _ = self.LSTM(jagged_input_tensor)\n",
    "        context_tensor, _  = torch.nn.utils.rnn.pad_packed_sequence(jagged_context_embedding_tensor, batch_first = True, total_length = input_tensor.size(1))\n",
    "        context_tensor = self.dropout(context_tensor)\n",
    "        return context_tensor\n",
    "\n",
    "\n",
    "    def _get_emission_scores(self, context_tensor, mask):\n",
    "        output_1 = self.classifier(context_tensor)\n",
    "        output_2 = self.classifier_2(torch.tanh(output_1))\n",
    "        return output_2\n",
    "\n",
    "    def _get_sentence_scores(self, GED_probs, labels):\n",
    "        emission_matrix = torch.sum(torch.mul(one_hot_encoding(labels),GED_probs), -1)\n",
    "        scores = torch.zeros_like(labels).to(self.device)\n",
    "        binary_converter = torch.tensor([[self.label_number],[1.0]]).to(self.device)\n",
    "        batch_size = labels.shape[0]\n",
    "        scores[:, 0] = emission_matrix[:,0].view(batch_size)\n",
    "        for i in range(1, GED_probs.shape[1]):\n",
    "            transition_labels = torch.matmul(labels[:, i-1:i+1], binary_converter).to(torch.long)\n",
    "            emission_scores = emission_matrix[:, i:i+1]\n",
    "            scores[:, i] = self.transitions[transition_labels].view(batch_size) + emission_scores.view(batch_size)\n",
    "        return scores\n",
    "\n",
    "    def _log_sum_exp(self, tensor):\n",
    "        max_tensor, _ = torch.max(tensor, -1)\n",
    "        tensor = tensor - max_tensor.unsqueeze(-1)\n",
    "        return max_tensor + torch.log(torch.sum(torch.exp(tensor), dim=-1))\n",
    "\n",
    "    def _TEMP_partition_function(self, GED_probs):\n",
    "        GED_probs = GED_probs[:1,:, :]\n",
    "        sum_TEMP = torch.zeros(1).to(self.device)\n",
    "        sum_TEMP_2 = torch.zeros(1).to(self.device)\n",
    "        sum_TEMP_3 = torch.zeros(1).to(self.device)\n",
    "\n",
    "        for i in [0,1]:\n",
    "            temp_labels = torch.FloatTensor([[i]]).to(self.device)\n",
    "            sum_TEMP_3 += torch.exp(torch.sum(self._get_sentence_scores(GED_probs[:,:1,:], temp_labels)))\n",
    "            for j in [0,1]:\n",
    "                temp_labels = torch.FloatTensor([[i,j]]).to(self.device)\n",
    "                sum_TEMP_2 += torch.exp(torch.sum(self._get_sentence_scores(GED_probs[:,:2,:], temp_labels)))\n",
    "                for k in [0,1]:\n",
    "                    for l in [0,1]:\n",
    "                        for m in [0,1]:\n",
    "                            temp_labels = torch.FloatTensor([[i,j,k,l,m]]).to(self.device)\n",
    "                            print(temp_labels, self._get_sentence_scores(GED_probs, temp_labels))\n",
    "                            sum_TEMP += torch.exp(torch.sum(self._get_sentence_scores(GED_probs, temp_labels)))\n",
    "        print(torch.log(sum_TEMP))\n",
    "        print(torch.log(sum_TEMP_2))\n",
    "        print(torch.log(sum_TEMP_3))\n",
    "        pdb.set_trace()\n",
    "\n",
    "    def _get_partition_function(self, GED_probs):\n",
    "        transitions = self.transitions.view(1,1,self.label_number,self.label_number)\n",
    "        GED_probs = GED_probs.unsqueeze(-1)\n",
    "        forward_matrix = GED_probs + transitions.permute(0,1,3,2)\n",
    "        Z = torch.zeros_like(GED_probs)\n",
    "        Z[:,0] = GED_probs[:,0]\n",
    "        for i in range(GED_probs.shape[1]-1):\n",
    "            update = self._log_sum_exp(forward_matrix[:,i+1] + Z[:, i].permute(0,2,1))\n",
    "            Z[:, i+1] = update.unsqueeze(-1)\n",
    "\n",
    "        Z = Z.squeeze(-1)\n",
    "        Z = self._log_sum_exp(Z)\n",
    "        return (Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
